1:了解pandas的函数get_dummies 函数的使用以及防止memory error 的方法
2：数据处理方式：
使用Pandas分块处理大文件
问题：今天在处理快手的用户数据时，遇到了一个差不多600M的txt文本，用sublime打开都蹦了，我用pandas.read_table()去读，差不多花了近2分钟，最后打开发现差不多3千万行数据。这仅仅是打开，如果要处理不知得多费劲。

解决：我翻了一下文档，这一类读取文件的函数有两个参数：chunksize、iterator

原理就是不一次性把文件数据读入内存中，而是分多次。


3：先对把数值数据进行标准化，也就是先对指定列进行标准化；
4：然后对数据的分类型数据进行one_hot编码
5：然后对发布日期进行编码，日期要不要处理：暂时想成有月份和没有月份的情况：
    1：月份处理成分类型数据，分别时4，5，6 三个月；
    2：不要发布时间这一栏；


以上是个人思路：
还有一种是仿照kaggle上的一种方法做log1p的方法；让数据都大致接近正太分布：
先仿照kaggle上做；


在进行one_hot 编码的时候很有可能会造成样本数据本身的类别很多很全，但是测试数据的类别不全，one_Hot编码之后测试数据维度很训练数据维度不一致导致很多问题：
由于类别过多，我们可以考虑在线one_hot编码之后采用降维操作，看看有没有方法将维度统一一下；


如此我们就先不考虑地址因素带来的影响，先只考虑
buildTypeId这个类别的影响；其他条件只加上longtitude，latitude，price，bedrooms；

顶一个取数据的函数，具有通用性，并将buildtyid改成pandas下的object类型的数据；均值填充；
将buildtypid类型改变存在需要将na值去掉才能改变；




