特征的使用方法：
    1：尽可能的找出对因变量有影响的所有自变量
    2:可用性评估：
        1：获取准度：对因变量是不是有一定程度的影响，
        2：覆盖率：对因变量所涉及的自变量是不是全面了
        3：准确率
        4：特征之间的覆盖率和相互影响因素

特征的获取方案：
    1：如何获取这些特征：获取方式很多一般用pandas,numpy tensorflow都能实现对数据文件的读取；
        但是对于网络文件的获取，可能会用到相应的网络文件库了，
    2：如何实现存储？：语言相关的文件存储方式，或者是存入数据库中，

特征的处理：
    1：特征清洗
        清洗一样样本，
        采样：
            数据不均衡
            样本权重
    2：预处理
        1单个特征
            归一化
            离散化
            dummy Coding
            缺失值：经常在跑模型时遇到的NAN lost错误，可以设置默认值，或者时把该条数据去掉
            数据变换：
                1log
                2：指数
                3：box-cox


        多个特征：
            降维
                PCA
                LDA
             特征选择：
                FIlter：
                    自变量和目标变量之间的关系
                    相关系数
                    卡方检验
                    信息增益。互信息
                 Wrapper：
                    通过目标函数（AUC/MSE）来决定是否加入一个变量
                    迭代：产生特征自己，评价：
                                        完全搜索，启发式搜索，随机搜索：GA,SA

                 Embedded:
                    思路：学习期滋生自动选择特征
                    正则化：
                        L1：Lasso
                        L2：Ridge
                     决策树：
                        熵和信息增益
                     深度学习

    3:衍生变量：对原始数据加工，生成的有商业意义的变量
last:特征监控：特征有效性： 特征重要性，权重
    特征监控：缉拿空重要特征----防止特征质量下降，影响质量下降，影响模型效果


离散特征：
    判断该离散值的出现次数
    如果某一列时离散特征，而且以一列有些值出现的次数非常少，（经常要对离散特征的值统计一下次数，才能判断多小才是小；
        我们把同意的值付给一个值，一如Rare，在kaggle泰坦尼克号里面，就可以这么对人姓名的title进行处理

    去掉取值变化小的特征，
        注意，对于离散的判断他的特征取值比率，如果大于95%以上可以不考虑，但是实际上，我们使用的数据一般不会出现
        对于连续的变量，只有将他离散化后才能使用；
     对于one_hot和label encoding ：one_hot+pca能很好地处理，对于类别数目太多的情况我们可以使用,PCA是用来降维的；
     如果one_hot的类别不多可以使用这样的方法；
     对于通过label处理过的数据用决策树，随机森林，xgboost能很好地处理

连续特征映射成离散型的：
    连续型特征经常在kaggle里面映射成离散性的特征，具体的分析如下：
    在工业界很少直接将连续的特征作为逻辑回归的特征输入，而是将连续特征离散化为一系列0,1特征交给逻辑回归模型，这样做的优势有一下几点；
        1：离散特征的怎加和减少都很容易，已于模型的快速迭代；
        2：稀疏矩阵向量内积乘法运算速度快，计算结果方便存储，容易扩展
        3:离散后的特征对于异常数据有很强的鲁棒性，比如一个特征是年龄大于30是一，否则为零，如果特征没有离散化，一个异常的数据300岁会对模型有很强的干扰；
        4：逻辑回归属于广义想i选哪个回归，表达能力受限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型映入了非线性，能狗提高模型的表达能力，加大拟合；
        5：离散化后可以进行特征交叉，（通过笛卡尔积实现的）能让M+N个变量变成M*N个变量，进一步引入了非线性，提高表达能力；加大拟合
        6:特征离散化后，模型更加的稳定，比如如果用户年龄离散化后，20-30作为一个区间，不会应为一个用户年龄增加一岁而变成另一个人，当然区间响铃出的样本会刚好相反，所以怎样划分区间是一门学问，
        7：特征离散化之后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险，
     有句话是是使用离散特征还是连续特征，其实是一个海量离散特征+简单模型，同少量连续特征+复杂模型的权衡，集可以离散化线性模型，也可以用连续特征加深度学习，也就是折腾特征和模型的区别；
        前者容易可以很多人一起做，后者目前还不够成熟；

是否归一化或者标准化：
    一般来说；连续特征要做归一化或者是标准化，但是假如你把特征映射成了离散特征，那这个归一化或者标准化就可做可不做了，（一般不做，不然会增加计算量）

    2：对于缺失值的处理常见方法：
        用平均数，总数，K最近邻平均数等来复制，要是离散性的特征，可以直接用pandasde fillna 来填充Missing，然后做labelencoding 或者one_hot_encoding
        特征的选择方法：
            1：这里的特征选择仅仅是筛选特征，不包括降维；
            2：按照周志华机器学习第11章，他把特征选择的方法中介成了三类，过滤式，包裹式，嵌入式，：但是过滤式和包裹式的处理方式挺少见的）
            3：过滤式和包裹式的处理方式是挺少见；
            4:过滤式方法先对数据集进行选择特征，然后再训练学习器，特征选择过程和后续的学习器无关，选择过程有前向搜索和后向搜索，评价标准有自己的信息增益，相关统计量和拟合优化判定系数ISLR）
            5:包裹式：包裹式特征选择是把学习器的性能作为特征自己的评价标准，一般来说，包裹式特征选择比过滤式更好（应为过滤式不考虑特征选择不考虑后续的学习器的性能，但是计算开销却比过滤式大得多；
            6:嵌入式：嵌入式特征选择式将特征选择和机器学习训练荣威一体，两者再同一个优化过程中完成，常见的有L1正则化和L振泽华，但是L1正则化能更容易获得一个稀疏的解

            单变量特征选择：
            单变量特征选择能够对每一个特征进行测试，衡量该特征和响应式变量之间的关系，根据得分去掉不好的特征；
                对于回归和分类问题可以采用卡方检验，皮尔逊相关系数等方式最特征进性测试；
                这类方法，已于运行，利于理解，通常对于理解数据有较好的效果，但对于特征优化，提高繁华能里来说不一定又掉，这种方法有许多该井的版本；


             降维的方法：
                一般经过one_hot encoding后，举证的维度可能会变的很大，此时一般需要进行降维的操作，常见的降维方式有PCA，

                特征构造：构造新特征的方法：
                    比如泰坦尼克里面把相抵姐妹数量和父母以及子女加起来构成一个新的特征，家庭的大小；

                    除了上面的直接相加，还有构造多项式特征（特征相乘）等；



 降维的常见方法：
  缺失值比率：
    该方法式基于包含太多缺失值的数据列包含有用信息的可能性较少，因此，可以将数据列缺失值大于某个阈值的列去掉，与之越高，降维的方法更为积极，即降维越少，
    低方差滤波：
          数据列变化非常小的列包含的信息量少，因此，所有的数据列方差晓得列被一处，需要注意的一点是：方差与数据范围相关的，因此再采用该方法时需要对数据做归一化；

          高相关滤波：
          高相关滤波认为当两列数据变化趋势相似的时，他们包含的信息也显示，这样
            使用相似列中的一列就可一乐，对于数值列之间的相似性通过相关稀疏来表示，对于名词类列相关系数可以计算皮尔逊卡方值来表示，相关系数大某个获知的两列直播啊咧一条，但是相关系数对数据敏感，也要做归一化处理；

           随机森林和组合数：再进行特征选择和构建有效的分类器时非常有用，能够得到数据属性评分，然后与其他数据相比较，那个属性才是最好的属性；

           组要成分分析（PCA):
           通过正交变化将原始的N维数据集变化到一个新的被称为主成分的数据集中，变化后的结果中，第一个主成分有最大的方差值，也需要做归一化处理，新的主成分不是由系统产生的，因此再进行PCA变化后会丧失数据的解释性，

           反向特征消除：
           反复迭代得到时分类器性能最好（再错误容忍率中）所需要的最少的特征；

           前向特征构造：
           是与前向特征构建相反的过程，他是从一个特征开始，每次训练添加一个让飞雷奇性能提升最大的特征，
           他俩适用于输入维数相对较小的数据集；
           还有：随机投影，肺腑矩阵分解，自动编码，卡方检测与信息增益，多为标定，相关分析，因子分析，聚类以及贝叶斯模型；






