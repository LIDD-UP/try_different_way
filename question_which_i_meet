#=AttributeError: 'list' object has no attribute 'name'

当loss值波动后不下降加上shuffle = True


LSTM 波士顿房价预测：
    crossent = softmax_loss_function(labels=target, logits=logit)
TypeError: ms_error() got an unexpected keyword argument 'labels'
官网查看，需要将我们传入的函数参数要是命名参数；
softmax_loss_function：函数（标签，logits） - >使用损失批处理而不是标准softmax（默认情况下，如果这是None）。
请注意，为避免混淆，函数需要接受命名参数。

AttributeError: module 'tensorflow' has no attribute 'scalar_summary'
tf.sub 改为了tf.subtract
tf.nn.seq2seq.sequence_loss_by_example 改为了tf.contrib.legacy_seq2seq.sequence_loss_by_example
此处再用pycharm的时候你先查看源代码看不了那是它做了限制，让你无法查看源代码；这时候需要查看官方文档；

tf.train.SummaryWriter改为：tf.summary.FileWriter

tf.merge_all_summaries()改为：summary_op = tf.summary.merge_all

tf.histogram_summary(var.op.name, var)改为：  tf.summary.histogram

KeyError: "The name 'input/xs' refers to an Operation not in the graph."
在有 with tf.name_scope('inputs'): 下的placehold 定义的名字时需要用到inputs/name
比如：        with tf.name_scope('inputs'):
            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')  # xs 有三个维度
            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')  # ys 有三个维度
            这时候如果想要通过graph.get_operation_by_name()


ValueError: Variable in_hidden/weights already exists, disallowed.
Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:
定义了两次，


one_hot 编码 sklearn 和 pandas 都有one_hot 独热编码方式：
但是推荐使用pandas 下的get_dummies

把列全部显示出来需要设置
pd.set_option('max_column',100) 就可一了；

用 pandas的getdummies result memeryError 这个可以通过把降入sparse=True 或者时将特征一个个的进行处理；

UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, input_dim=6, kernel_regularizer=<keras.reg...)`
  model.add(Dense(1,input_dim=train.shape[1],W_regularizer=l1(0.001)))
  ：也就是把W_regularizer改为kernel_regularizer

TypeError: Singleton array array(StandardScaler(copy=True, with_mean=True, with_std=True),
      dtype=object) cannot be considered a valid collection.
      :这是由于在训练期间应该使用fit_transform 不应该使用fit，fit和train有点类似但是不太相同；fit相当于是适合得意思；


GridSearchCV和RandomizedSearchCV得区别是一个是暴力组合参数，选择最好得一个是给定一个范围选择最好得；


已经用到得数据处理技术：
    1：数据转换：log变化法
    2：boxplot箱图离群点处理
    3：scatter坐标点数据处理；
    4：标准化
    5：归一化
    6：网格搜索：gridSearchCV
    7：get_dummies(one_hot 编码）
    8:label_encoding(标签编码）
    9：Shapiro-Wilk  正太分布检验法；


random 模块下的sample才是在一个列表里面返回若干个数
chocie是在一个list中返回一个数，随机的；


输入函数有问题，或者是输入的数据未去除nan值；，需要加上np.array(data[column])
tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.


出现长尾分布的原因很大程度式由于数据有离群点或者是异常值需要去除；

data = data.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)) if x.dtype!= 'object' else x):数据标准化和还原


pd.factorize,与pd.get_dummies 有类似的作用，都是编码，但是factorize类似玉标签编码，get_dummies类似于one_hot编码；


pandas .


UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.
  warnings.warn("The 'normed' kwarg is deprecated, and has been "


  KeyError: 'the label [9] is not in the [index]'
  这是由于我在做数据处理过程中，有些数据被去除掉了，但是索引没有重置，所以需要重置索引，：
    方法：reindex，但是在适用这个方法的时候需要参数，新的索引的列表，所有应该为：

    data_feature = data_feature.reindex([x for x in range(len(data_feature))])



# 又遇到一个莫名其妙的bug当用dnn的模型的时候老是会遇到NaN的错误问题；


关于autoML的使用问题：包括特征自动化等等；

特征备份：
    # 'longitude','latitude','bedrooms','price','washrooms','bedroomsPlus','lotDepth','lotFront',
    # 'kitchens','kitchensPlus','parkingSpaces','room1Length','room1Width','room2Length',
    # 'room3Length', 'room3Width', 'room4Length', 'room4Width', 'room5Length', 'room5Width',
    # 'room6Length','room6Width',   'room7Length',    'room7Width',  'room8Length',
    # 'room8Width',  'room9Length',   'room9Width',         'rooms',
    # 'taxes',  'garageSpaces',  'totalParkingSpaces'


ValueError: 'airConditioning_Central Air' is not a valid scope name:column的命名的错误，还是需要先labelencode之后再get_dummies;


在使用panda的一些函数的时候，很多时候都需要将他看作是一个数组，应为他会加上该字段或者是该列的类型等一些信息：
如：data[column].mode() 输出的时候会有一个类型的问题：所以需要用到下标获取才能获取真正的值；

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
        需要将程序写到main函数里面

当遇到出现import error的时候


pytorch 安装出现错误 由于pytoch不支持pip安装了；


Can't instantiate abstract class Supervised with abstract methods evaluate, fit, predict
这个问题就是它相当于是一个接口，必须继承之后实现，不能直接实例化；


非参数检验；和参数检验；利用的假设法；
非参数检验的适用情况：
非参数检验适用于以下三种情况：
　　①顺序类型的数据资料，这类数据的分布形态一般是未知的；
　　②虽然是连续数据，但总体分布形态未知或者非正态，这和卡方检验一样
    ，称自由分布检验；③总体分布虽然正态，数据也是连续类型，但样本容量极小，
    如10以下（虽然T检验被称为小样本统计方法，但样本容量太小时，代表性毕竟很差，
    最好不要用要求较严格的参数检验法）。
　　非参数检验(Nonparametric tests)是统计分析方法的重要组成部分，
    它与参数检验共同构成统计推断的基本内容。参数检验是在总体分布形式已知的情况下
    ，对总体分布的参数如均值、方差等进行推断的方法。

data = data[data.buildingTypeId.isin([1,3,6])]


# _x['buildingTypeId'] = _x['buildingTypeId'].astype('str') 只要有astype就错误，主要是在用到特征工程的时候
vocabulary_list 只接受只接受字符串；不能是浮点数；


解决模型欠你合或者是过拟合的方法：
    https://blog.csdn.net/hans__yang/article/details/72629403
    欠拟合会出现高bias的问题，过拟合会导致高方差的问题：
    模型处于过拟合还是欠拟合，可以通过画出误差趋势图来观察：目前不知道怎么画的，

特征选择的方法：
    1：PCA：https://blog.csdn.net/puredreammer/article/details/52255025
    2：单因子分析，多因子分析
    3：卡方检验
    4：皮尔逊相关系数：只能检测出线性相关性，无法检测出非线性关系
    5：方差检测法：取方法较大的特征

数据挖掘课程实战：癌症数据挖掘：https://study.163.com/course/introduction.htm?courseId=1005269003#/courseDetail?tab=1

正太分布的检验方法：
    https://www.cnblogs.com/webRobot/p/6760839.html


:sklearn 的特征选择方法：  xgboost利用标签编码有时也会
    比one_hot好一点，最主要是节省时间，况且xgboost是基于决策树的模型，所以one_hot
        编码的必要性不是很大；
        imputer
        sklearn.preprocessing.Imputer
        sklearn.feature_selection.f_regression(X, y, center=True)
我当然不满意啦，一直想着怎么能提高准确率呢？后来就想到了可以利用一下scikit这个库啊！在scikit中包含了一个特征选择的模块sklearn.feature_selection，而在这个模块下面有以下几个方法：
Removing features with low variance（剔除低方差的特征）
Univariate feature selection（单变量特征选择）
Recursive feature elimination（递归功能消除）
Feature selection using SelectFromModel（使用SelectFromModel进行特征选择）
我首先想到的是利用单变量特征选择的方法选出几个跟预测结果最相关的特征。根据官方文档，有以下几种得分函数来检验变量之间的依赖程度：
对于回归问题: f_regression, mutual_info_regression
对于分类问题: chi2, f_classif, mutual_info_classif
由于这个比赛是一个回归预测问题，所以我选择了f_regression这个得分函数（刚开始我没有注意，错误使用了分类问题中的得分函数chi2，导致程序一直报错！心很累~）


plt.tight_layout() 能自动解决多个子图相互重叠的问题，自解决标签，标题重合或者未完全显示问题；自动调整的；


pandas 读入pandas数据再写成csv文件的时候，文件大小会改变；


2018-09-07 11:08:55.856714: W T:\src\github\tensorflow\tensorflow\core\framework\op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: Failed to create a NewWriteableFile: ./save/file\model.ckpt-0_temp_5156c8ea3fc149db9763a4cc962b8c85/part-00000-of-00001.data-00000-of-00001.tempstate17719047275205881367 : ϵͳ�Ҳ���ָ����·����
; No such process
这是需要将model_dir 设置成为绝对地址


tensorflow报ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape
这是由于内存导致的错误，可以将数据的输入分批次输入；

还有对于下标的重值，直接用reindex不起作用，需要用到reset_index


通过下表来区分数据：
train_2 = train_data[~(train_data.index.isin(train.index))]

运算符号：~的合理应用；


机器学习中的分布对模型的影响；








