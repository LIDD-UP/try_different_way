1:了解pandas的函数get_dummies 函数的使用以及防止memory error 的方法
2：数据处理方式：
使用Pandas分块处理大文件
问题：今天在处理快手的用户数据时，遇到了一个差不多600M的txt文本，用sublime打开都蹦了，我用pandas.read_table()去读，差不多花了近2分钟，最后打开发现差不多3千万行数据。这仅仅是打开，如果要处理不知得多费劲。

解决：我翻了一下文档，这一类读取文件的函数有两个参数：chunksize、iterator

原理就是不一次性把文件数据读入内存中，而是分多次。


3：先对把数值数据进行标准化，也就是先对指定列进行标准化；
4：然后对数据的分类型数据进行one_hot编码
5：然后对发布日期进行编码，日期要不要处理：暂时想成有月份和没有月份的情况：
    1：月份处理成分类型数据，分别时4，5，6 三个月；
    2：不要发布时间这一栏；


以上是个人思路：
还有一种是仿照kaggle上的一种方法做log1p的方法；让数据都大致接近正太分布：
先仿照kaggle上做；


