Code review procedure
    1:关于cnn预测模型的重构
    关于cnn模型的知识：
    1：分为三层，卷积层，池化层，全连接层，卷积层和池化层可以有多层，
    1:cnn最重要的一步就是要将输入的形状reshape成一个三维的数据形式，通常是一个正方形一般这样[-1,x,x,1]这样的形式；
    存在的问题：
        1：padding的作用
        2：stride的作用：
        3：为什么要reshape成正方形；
        4：还有weight的形成方式和计算方法
        5：bias的生成规则和计算方法
        6：tf.conv2d 的计算方法；（矩阵计算）
        7：池化的计算方法和作用，对原始矩阵有什么影响，好像这个还要影响weight的生成；
        8：池化中ksize和stride，padding的相互联系和设置方式；
        9：对于多层的卷积应该怎样设置，卷积层的设置方法；多层卷积的作用；
        10：全连接层的设计规则和矩阵应该遵循的设计方案；全连接层的作用
        python 单下划线和双下划线：类似于java中的protect 和 private

        一次卷积和池化过程包括
            1：卷积
            2：池化
            3：dropout
            4：全连接层

        由于对rnn不熟悉，先暂停对cnn得重构，
        rnn也暂停重构；先把神经网络系统的学习一遍之后在进行重构；


        对ML进行重构：
            遇到第一个问题就是项目模块化得问题：
            python项目下得根py文件，就算把项目下加上__init__.py文件也无法导入（在命令行下无法导入，但是在pycharm中就可以导入，
            这个问题暂时无法解决：
            但是在更目录下得模块就能导入，很奇怪，拿到还要在创建一个package专门获取根路径？
            而且，无论在根目录下创部创建__init__.py 文件，根目录下得模块都能导入；

        此处需要注意os.getcwd()是获取系统工作目录，而os.path.dirname(os.path.abspath(__file__)是文件目录；

        对于机器学习的框架都是调用的sklearn包，只需要进行fit data就行了；还有就是之前一部分的数据处理部分
        ：可以设计成三部分函数：
            1：数据处理
            2：数据转换
            3：训练
                交叉验证，
            4：预测
            5：数据可是化
            6：结果保存；


统一以下训练的整个过程
    1：包的导入和设置如pandas的控制台显示
    2：定义类
        1：构造函数
        2：训练和预测数据的导入（此处的数据不能够合并，预测数据不能进行处理，只能进行特征的选择），
            同时可以把训练数据进行预测，去除掉一些差距比较大的数据）比较训练数据不同维度的分布情况
        3：数据的预处理部分
            1：图形展示数据分布函数
            2：数据处理部分函数
            3：再次调用数据分布函数观察处理效果；
        4：训练函数(这里可能需要将不同的框架分隔成一个新的类，因为不同的框架可能导致前面数据处理方式不同）
            1：auto_ml
            2: xgbooot
            3: randomforest
            4: bagging
            5: ridge
            6: lasso
            7: keras
            8: tensorflow
        5：预测函数
        6:结果保存函数
        7：结果导入进行预测（应该是使用pickle）
    3：main函数的运行
        1：数据导入
        2：数据处理
        3：训练
        4：预测


    参数调节的方式：
        1：利用sklearn下的
            from skearn.model_selection import cross_val_score
            这种方式就只能一次调节一个参数，不能使用网格搜索的方式，但是也可以通过遍历的方式实现网格搜索；
            通过-cross_val_score 找到最小的情况：
                for n in k_range:
                    knn = KNeighborsClassifier(n)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV
                    scores = cross_val_score(knn,train_X,train_y,cv=10,scoring='accuracy')  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值
                    cv_scores.append(scores.mean())
                def rmse_cv(model):
                        rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring="neg_mean_squared_error", cv = 5))
                        return(rmse)
                model_ridge = Ridge()


                alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]
                cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean()
                            for alpha in alphas]
                cv_ridge = pd.Series(cv_ridge, index = alphas)
                cv_ridge.plot(title = "Validation - Just Do It")
                plt.xlabel("alpha")
                plt.ylabel("rmse")
                plt.show()

                还有这种方式；

        2:还有一种是使用xgboost自带的：



