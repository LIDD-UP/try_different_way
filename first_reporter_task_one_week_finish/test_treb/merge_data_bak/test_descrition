style:34,23

style+community:
0.3334953816237239
0.24052017501215361
0.16541079241614
0.2605736509479825
22.817073620692817


community:
0.3217665615141956
0.2499393351128367
0.16549381218150935
0.26280029119145837
23.031641168722796


突然发现；treb本身的预测结果作为特征作为输入的时候效果要更好一些，并且误差在10天内的天数到了%34左右；
如果不加的化：大致在%25左右；

# 这是原始的数据形式：
0.3420238095238095
0.2695238095238095
0.07547619047619047
0.31297619047619046
28.07547619047619
'''

'''
my predictions
0.26134169884169883
0.2592905405405405
0.19063706563706564
0.288730694980695
24.81238204908187
'''
'''
0.3322876447876448
0.2548262548262548
0.16011100386100385
0.2527750965250965
22.444079292096685


巨大的数据预测方面的突破，发现了一个规律根据训练数据自生预测自生可能会有突破情况；
第一次
数据情况：
(19163, 92)
(19163, 10)
(18909, 10)
(18909, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

预测结果：
0.26291701794532096
0.26062868842587017
0.19149704925930386
0.284957244369505
24.718915022526478

第二次：
筛选掉：大于误差在30以上的数据；
数据变化：
((15594, 12)
(15594, 10)
(15594, 10)
(15594, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：
0.34818740214380345
0.23882933879320728
0.14006985426954113
0.27291340479344817
24.076015467473173

第三次也是筛选掉误差大于30的；
(15025, 12)
(15025, 10)
(15025, 10)
(15025, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：
0.36637359990364926
0.23882933879320728
0.11682524388775141
0.277971817415392
24.063211034209445

第四次筛选掉误差大于20的数据：
(13285, 12)
(13285, 10)
(13285, 10)
(13285, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果显示：
0.4038299409851861
0.21185113814283993
0.09249668794411658
0.2918222329278574
24.30003670946627


第五次帅选数据：将大于15的筛选掉；
(11692, 12)
(11692, 10)
(11692, 10)
(11692, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果显示：
0.4171986029146092
0.1986029146091774
0.08394556184511623
0.3002529206310972
24.617138323117118


以上是使用的78月份的数据进行测试的；
还有很多种测试的结果：
首先是训练数据的保留情况：上面78月份的测试结果显示：如果直接选取最小的结果可能导致泛化能力不强；
所以应该比较合理的方法是先从大小的方向进行筛选数据；

但是为了测试数据量对模型的影响情况，所以要保持情况不变，数据改变；
现在测试678月份的数据：
相同的测试方法：
1：
(29849, 92)
(29849, 10)
(29442, 10)
(29442, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：可以看出数据量数据量一定程度的增加对预测的效果还是有所提升的：
0.2683367457545465
0.2807419005178851
0.19294231000843068
0.25797904371913766
24.496070339093077

第二次：
(25133, 12)
(25133, 10)
(25133, 10)
(25133, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：
0.35878598097073344
0.2643622786944478
0.0997229916897507
0.27712874864506803
24.125307962835


第三次：
(24239, 12)
(24239, 10)
(24239, 10)
(24239, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果：
0.3857641816211008
0.24412862820667228
0.08551126099000361
0.2845959291822233
24.21903106940305

第四次：
(21669, 12)
(21669, 10)
(21669, 10)
(21669, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果：
0.43381910152956765
0.19390581717451524
0.0714199686860171
0.30085511260990005
24.56597959037633


第五次：
(19174, 12)
(19174, 10)
(19174, 10)
(19174, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果：
0.45320968324701916
0.16223051908948574
0.07575575093339756
0.30880404673009754
24.946653405944943

测试完678月份的数据之后可以看出，数据量对于预测结果还是有比较大的影响的；

接下来测试半年的数据情况：也就是3到8月份的数据:
1:
(54494, 92)
(54494, 10)
(53522, 10)
(53522, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)

结果：
0.26640973142237745
0.3194026255570276
0.1584969288209081
0.25569071419968686
24.451007975909448


2：
46496, 12)
(46496, 10)
(46496, 10)
(46496, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：
0.3855233048295797
0.2524388775141515
0.07997109478501746
0.28206672287125134
24.454756729748972

3：
(44861, 12)
(44861, 10)
(44861, 10)
(44861, 10)
sys:1: DtypeWarning: Columns (25,91) have mixed types. Specify dtype option on import or set low_memory=False.
(8415, 92)
(8415, 10)
(8303, 10)
(8303, 10)
结果：
0.41033361435625676
0.22401541611465736
0.07455136697579189
0.291099602553294
24.686564076453624

第四次：
0.46513308442731544
0.15343851619896423
0.06889076237504517
0.31253763699867515
25.373549918733303

5：

0.4721185113814284
0.13657714079248465
0.0714199686860171
0.31988437914006984
25.77730350753966

总结上述的测试结果：数据量对模型的影响还是比较大的，
并且如果对于训练数据的筛选太低的化可能出现两级分化的情况，即小于10和大于30的数据越来越多；
所以下面用半年的数据进行测试：3to8结果：
    目的：测试出最佳的筛选比例：
1:100:

0.2789353245814766
0.32145007828495725
0.14488739009996388
0.2547272070336023
24.38337412676466

2:100

0.28086233891364565
0.31988437914006984
0.14416475972540047
0.255088522220884
24.383351630365134

3:100
0.27749006383234975
0.32145007828495725
0.14464651330844272
0.25641334457425025
24.42300025690008

这里如果发现继续筛选如果误差有所提升就不再进行筛选；换个筛选进行：
100筛选两次就可以了；
80:
0.28820908105504034
0.3216909550764784
0.13188004335782247
0.2582199205106588
24.36536016042701

70:
0.2998916054438155
0.3212092014934361
0.11815006624111767
0.26074912682163076
24.361415524317906


                        不行的：
                            60:
                            0.3135011441647597
                            0.3150668433096471
                            0.10610622666506082
                            0.26532578586053235
                            24.41741522136947

                            跳过6直接5不行：
                            0.3316873419246056
                            0.30434782608695654
                            0.09117186559075033
                            0.2727929663976876
                            24.500562578026937

65：
0.30892448512585813
0.3147055281223654
0.11393472238949777
0.2624352643622787
24.34759821542039

63:
0.3112128146453089
0.3135011441647597
0.11044200891244128
0.26484403227749004
24.387008319084263

60:
0.3157894736842105
0.31374202095628084
0.10297482837528604
0.26749367698422255
24.445031717974178

50:
0.33240997229916897
0.3062748404191256
0.08647476815608816
0.27484041912561724
24.542660148319214

            40：
            0.3632422016138745
            0.27749006383234975
            0.07696013489100326
            0.28230759966277247
            24.708028782538214

45:
0.34927134770564855
0.2900156569914489
0.08322293147055282
0.27749006383234975
24.629237741012968

40:
24.73800063716149
0.365891846320607
0.274358665542575
0.07696013489100326
0.2827893532458148
24.73800063716149

40:
0.3709502589425509
0.27098639046127904
0.07455136697579189
0.28351198362037816
24.76371741211588

37:

0.38215102974828374
0.25653378297001084
0.07455136697579189
0.2867638203059135
24.8631915904866

35
0.3938335541370589
0.24509213537275684
0.07081777670721426
0.29025653378297
24.9576087800276

30:
0.4184029868722149
0.21558472841141757
0.06672287125135493
0.29928941346501264
25.150888742086014


20:
0.4633265084909069
0.15765386005058413
0.06587980248103095
0.313139828977478
25.651030404702965
10
0.47235938817294953
0.13260267373238588
0.07190172226905937
0.3231362158256052
26.12112513958769


 "longitude",
        "latitude",
        # "city",
        # "province",
        "price",
        "tradeTypeId",
        # # "listingDate",
        "buildingTypeId",
        "bedrooms",
        "bathroomTotal",
        # 'postalCode',
        'daysOnMarket',
        'ownerShipType',
        # 'projectDaysOnMarket',
        'district',


接下来是对特征进行测试，将特征数进一步的做增加，边增加边进行训练数据的筛选；
用tableau分析了一下，预测的值普遍偏小，且大多数分布在30以内；
基本上除了预测值以外，其他字段的分布情况基本相似；




继续测试了10月份的数据发现采用那样的规则也是可行的，能够增加10天以内误差的数据；
但是也存在的问题就是总体的误差不会有所下降，他是呈现一种先降再升的状态，到达升的状态的时候30天以上的那一部分的占比也会随着升上去
可以总结成两级分化的状态，即中间部分的10，20的那部分的数据慢慢的变少来补上10天以内和大于30天的数据，但是
相比较30天补充的而言，10天以内的增加是比较快的；30天以上的增加是比较慢的；





接下来添加特征，然后进行筛选；
还是用3to8月份的数据
1:首次运行：

0.2684280371576789
0.3231994209192906
0.15393895524188683
0.2544335866811437
24.295154635962884

300
300
250
24.303884689159855
0.26830739534322595
0.3210278682591386
0.15731692604656775
0.2533478103510677
24.303884689159855

200:
0.27192664977681263
0.3222342864036675
0.15176740258173482
0.254071661237785
24.257094744391114

150:
0.27084087344673663
0.32295813729038486
0.1523706116539993
0.2538303776088792
24.241661394385954


100
0.2810954276752322
0.3225962118470262
0.14283990831222101
0.25346845216552055
24.200541240744144

90:

0.28616238388225357
0.3188563155989866
0.14139220653878634
0.2535890939799735
24.18635516420379

80:
0.2920738327904452
0.3198214501146097
0.13306792134153697
0.25503679575340815
24.188711535817017

70:
0.303172879720111
0.31921824104234525
0.11750512727711425
0.2601037519604295
24.15243776820511
70:
0.30401737242128124
0.3198214501146097
0.11774641090602003
0.25841476655808904
24.12956577187813
60:
0.31282422487634215
0.31608155386657016
0.10628543853299553
0.2648087827240922
24.223657892443242


0.3146338520931355
0.3146338520931355
0.10580287127518398
0.26492942453854507
24.21842641838498


50:
0.33429846784895645
0.303172879720111
0.09011943539630836
0.2724092170346242
24.324778682806766

40:
0.3689226685969357
0.2745807696947762
0.07745204487875498
0.27904451682953313
24.51067473181109

30:
0.40933767643865365
0.22946073108939558
0.07129931234165762
0.2899022801302932
24.697998132589486

#20:
0.4629026420557365
0.16153938955241887
0.06731813246471227
0.30823983592713233
25.220359612408643



9月份的别人treb去除掉drop值计算的：
0.3426227530462058
0.2705995898178309
0.07588370129086741
0.31089395584509594
27.684039087947884

自己的：
0.4238146941730004
0.198455784775003
0.06936904331041138
0.30836047774158526
25.460135931732253


10月份别人treb的去掉drop值计算的：
0.3623836576816149
0.2936057053064185
0.0911398525323341
0.2528707844796325
27.450380756678353

自己的：
0.4824126677142512
0.155445424876103
0.07506345944639188
0.28707844796325394
26.972955967716274


目前keras结果：
9月份：最好41：
10月份：最好46；
相比较与xgboost还是有一定的差距；
接下来需要简化keras，对特征进行简化，对数据筛选进行优化：

看了一下auto_ml的源码，发现它的深度学习方面的东西也是用的keras包了一层；

其实再剔除训练数据的时候，还可以把训练数据拆分开来之后进行预测，然后不断的剔除预测效果不好的数据；


之前的逻辑有一定的问题，如果重原始的模型开始基础上再进行训练，可能有一定的问题：具体还是要以测试
情况定：

finish the auto shell program






